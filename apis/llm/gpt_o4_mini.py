from flask import jsonify, request, g, make_response
from apis.utils.tokenService import TokenService
from apis.utils.databaseService import DatabaseService
import logging
import pytz
from datetime import datetime
from apis.utils.llmServices import gpt_o4_mini_service  # Import the multimodal service function
from apis.utils.fileService import FileService
import os
import tempfile
import base64

# CONFIGURE LOGGING
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Allowed image file extensions and MIME types
ALLOWED_IMAGE_EXTENSIONS = {
    'png': 'image/png',
    'jpg': 'image/jpeg',
    'jpeg': 'image/jpeg'
}

from apis.utils.config import create_api_response

def gpt_o4_mini_route():
    """
    Consumes 2 AI credits per call
    
    OpenAI GPT-o4-mini LLM model for multimodal content generation with enhanced reasoning capabilities.
    Supports image file references and advanced reasoning parameters for enhanced visual analysis.
    
    ---
    tags:
      - LLM
    parameters:
      - name: X-Token
        in: header
        type: string
        required: true
        description: Authentication token
      - name: X-Correlation-ID
        in: header
        type: string
        required: false
        description: Unique identifier for tracking requests across multiple systems
      - name: body
        in: body
        required: true
        schema:
          type: object
          required:
            - user_input
          properties:
            system_prompt:
              type: string
              description: System prompt to control model behavior
              default: "You are a helpful AI assistant"
            user_input:
              type: string
              description: Text for the model to process
            json_output:
              type: boolean
              default: false
              description: When true, the model will return a structured JSON response
            file_ids:
              type: array
              items:
                type: string
              description: Array of image file IDs to process with the model (supports PNG, JPG, JPEG only)
            context_id:
              type: string
              description: ID of a context file to use as an enhanced system prompt (optional)
            formatting_reenabled:
              type: boolean
              default: true
              description: Enable enhanced formatting capabilities
            reasoning_effort:
              type: string
              enum: ["high", "medium", "low"]
              default: "medium"
              description: Level of reasoning effort for enhanced analysis
            generate_summary:
              type: string
              enum: ["none", "detailed"]
              default: "none"
              description: Generate detailed summary of reasoning process
            max_completion_tokens:
              type: integer
              default: 4000
              minimum: 1
              maximum: 16000
              description: Maximum number of tokens to generate in the completion
    produces:
      - application/json
    responses:
      200:
        description: Successful model response
        schema:
          type: object
          properties:
            response:
              type: string
              example: "200"
            message:
              type: string
              example: "I'll help you with that question. Based on the information provided..."
            user_id:
              type: string
              example: "user123"
            user_name:
              type: string
              example: "John Doe"
            user_email:
              type: string
              example: "john.doe@example.com"
            model:
              type: string
              example: "gpt-o4-mini"
            client_used:
              type: string
              example: "primary"
              description: Which client was used (primary, secondary, tertiary)
            context_used:
              type: string
              example: "ctx_123e4567-e89b-12d3-a456-426614174000"
              description: Context identifier used in this request (or 'none' if no context)
            prompt_tokens:
              type: integer
              example: 125
              description: Number of tokens pass in the prompt
            completion_tokens:
              type: integer
              example: 84
              description: Number of tokens generated by the llm
            total_tokens:
              type: integer
              example: 209
              description: Total number of tokens used
            cached_tokens:
              type: integer
              example: 0
              description: Number of cached tokens (if supported by model)  
            files_processed:
              type: integer
              example: 1
              description: Number of image files processed in the request
            file_processing_details:
              type: object
              properties:
                images_processed:
                  type: integer
                  example: 1
      400:
        description: Bad request
        schema:
          type: object
          properties:
            response:
              type: string
              example: "400"
            message:
              type: string
              example: "Missing required fields: user_input"
      401:
        description: Authentication error
        schema:
          type: object
          properties:
            error:
              type: string
              example: "Authentication Error"
            message:
              type: string
              example: "Token has expired"
      500:
        description: Server error
        schema:
          type: object
          properties:
            response:
              type: string
              example: "500"
            message:
              type: string
              example: "Internal server error occurred during API request"
    """
    # Get token from X-Token header
    token = request.headers.get('X-Token')
    if not token:
        return create_api_response({
            "error": "Authentication Error",
            "message": "Missing X-Token header"
        }, 401)
    
    # Validate token from database
    token_details = DatabaseService.get_token_details_by_value(token)
    if not token_details:
        return create_api_response({
            "error": "Authentication Error",
            "message": "Invalid token - not found in database"
        }, 401)
    
    # Store token ID and user ID in g for logging and balance check
    g.token_id = token_details["id"]
    g.user_id = token_details["user_id"]  # This is critical for the balance middleware
    
    # Check if token is expired
    now = datetime.now(pytz.UTC)
    expiration_time = token_details["token_expiration_time"]
    
    # Ensure expiration_time is timezone-aware
    if expiration_time.tzinfo is None:
        johannesburg_tz = pytz.timezone('Africa/Johannesburg')
        expiration_time = johannesburg_tz.localize(expiration_time)
        
    if now > expiration_time:
        return create_api_response({
            "error": "Authentication Error",
            "message": "Token has expired"
        }, 401)
    
    # Validate token with Microsoft Graph
    is_valid = TokenService.validate_token(token)
    if not is_valid:
        return create_api_response({
            "error": "Authentication Error",
            "message": "Token is no longer valid with provider"
        }, 401)
        
    # Get user details
    user_id = token_details["user_id"]
    user_details = DatabaseService.get_user_by_id(user_id)
    if not user_details:
        return create_api_response({
            "error": "Authentication Error",
            "message": "User associated with token not found"
        }, 401)
    
    # Get request data
    data = request.get_json()
    if not data:
        return create_api_response({
            "response": "400",
            "message": "Request body is required"
        }, 400)
    
    # Validate required fields
    required_fields = ['user_input']
    missing_fields = [field for field in required_fields if field not in data]
    if missing_fields:
        return create_api_response({
            "response": "400",
            "message": f"Missing required fields: {', '.join(missing_fields)}"
        }, 400)
    
    # Extract parameters with defaults
    system_prompt = data.get('system_prompt', 'You are a helpful AI assistant')
    user_input = data.get('user_input', '')
    json_output = data.get('json_output', False)
    file_ids = data.get('file_ids', [])
    context_id = data.get('context_id')
    formatting_reenabled = data.get('formatting_reenabled', True)
    reasoning_effort = data.get('reasoning_effort', 'medium')
    generate_summary = data.get('generate_summary', 'none')
    max_completion_tokens = data.get('max_completion_tokens', 4000)
    
    # Validate and clean file_ids - filter out empty strings and non-string values
    if file_ids and isinstance(file_ids, list):
        file_ids = [file_id.strip() for file_id in file_ids if isinstance(file_id, str) and file_id.strip()]
    else:
        file_ids = []
    
    # Validate and clean context_id - treat empty strings as None
    if context_id and isinstance(context_id, str):
        context_id = context_id.strip()
        if not context_id:  # Empty string after stripping
            context_id = None
    else:
        context_id = None
    
    # Validate max_completion_tokens
    if not isinstance(max_completion_tokens, int) or max_completion_tokens <= 0:
        return create_api_response({
            "response": "400",
            "message": "max_completion_tokens must be a positive integer"
        }, 400)
    
    if max_completion_tokens > 16000:
        return create_api_response({
            "response": "400",
            "message": "max_completion_tokens cannot exceed 16000"
        }, 400)
    
    # Validate reasoning_effort
    valid_reasoning_efforts = ["high", "medium", "low"]
    if reasoning_effort not in valid_reasoning_efforts:
        return create_api_response({
            "response": "400",
            "message": f"reasoning_effort must be one of: {', '.join(valid_reasoning_efforts)}"
        }, 400)
    
    # Validate generate_summary
    valid_summary_options = ["none", "detailed"]
    if generate_summary not in valid_summary_options:
        return create_api_response({
            "response": "400",
            "message": f"generate_summary must be one of: {', '.join(valid_summary_options)}"
        }, 400)
    
    try:
        # Log API usage
        logger.info(f"GPT-o4-mini API called by user: {user_id}")
        
        # Apply context if provided (same as other implementations)
        if context_id:
            from apis.llm.context_integration import apply_context_to_system_prompt
            enhanced_system_prompt, error = apply_context_to_system_prompt(system_prompt, context_id, g.user_id)
            if error:
                logger.warning(f"Error applying context {context_id}: {error}")
                # Continue with original system prompt but log the issue
                enhanced_system_prompt = system_prompt
        else:
            enhanced_system_prompt = system_prompt
        
        # Log request type for debugging
        if file_ids and len(file_ids) > 0:
            logger.info(f"Multimodal request with {len(file_ids)} files")
            # Check for too many files to prevent context overflow
            if len(file_ids) > 15:  # Conservative limit for o4-mini model
                return create_api_response({
                    "response": "400",
                    "message": "Too many files. GPT-o4-mini can process a maximum of 15 image files per request."
                }, 400)
        else:
            logger.info("Text-only request (using multimodal service)")

        # Use the multimodal service with enhanced reasoning parameters
        service_response = gpt_o4_mini_service(
            system_prompt=enhanced_system_prompt,  # Use enhanced prompt with context
            user_input=user_input,
            json_output=json_output,
            file_ids=file_ids,  # Will be empty list for text-only requests
            user_id=user_id,
            formatting_reenabled=formatting_reenabled,
            reasoning_effort=reasoning_effort,
            generate_summary=generate_summary,
            max_completion_tokens=max_completion_tokens
        )
        
        if not service_response["success"]:
            logger.error(f"GPT-o4-mini API error: {service_response['error']}")
            status_code = 500 if not str(service_response["error"]).startswith("4") else 400
            return create_api_response({
                "response": str(status_code),
                "message": service_response["error"]
            }, status_code)
        
        # Prepare successful response with user details - consistent structure
        response_data = {
            "response": "200",
            "message": service_response["result"],
            "user_id": user_details["id"],
            "user_name": user_details["user_name"],
            "user_email": user_details["user_email"],
            "model": service_response["model"],
            "client_used": service_response.get("client_used"),
            "context_used": context_id if context_id else "none",
            "prompt_tokens": service_response["prompt_tokens"],
            "completion_tokens": service_response["completion_tokens"],
            "total_tokens": service_response["total_tokens"],
            "cached_tokens": service_response.get("cached_tokens", 0),
        }
        
        # Always include file processing details for consistency
        response_data["files_processed"] = service_response.get("files_processed", 0)
        
        if "file_processing_details" in service_response:
            response_data["file_processing_details"] = service_response["file_processing_details"]
        else:
            # Provide consistent structure even for text-only requests
            response_data["file_processing_details"] = {
                "images_processed": 0,
                "file_ids_processed": []
            }
        
        
        return create_api_response(response_data, 200)
        
    except Exception as e:
        logger.error(f"GPT-o4-mini API error: {str(e)}")
        status_code = 500 if not str(e).startswith("4") else 400
        return create_api_response({
            "response": str(status_code),
            "message": str(e)
        }, status_code)

def register_llm_gpt_o4_mini(app):
    """Register routes with the Flask app"""
    from apis.utils.logMiddleware import api_logger
    from apis.utils.balanceMiddleware import check_balance
    from apis.utils.usageMiddleware import track_usage
    from apis.utils.rbacMiddleware import check_endpoint_access
    
    app.route('/llm/gpt-o4-mini', methods=['POST'])(track_usage(api_logger(check_endpoint_access(check_balance(gpt_o4_mini_route)))))